{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 05 - Model Evaluation\n",
        "\n",
        "This notebook evaluates model performance, generates visualizations, and applies SHAP interpretability analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from src.data_processing import DataProcessor\n",
        "from src.feature_engineering import FeatureEngineer\n",
        "from src.model_training import ModelTrainer\n",
        "from src.evaluation import ModelEvaluator\n",
        "\n",
        "# Prepare data\n",
        "processor = DataProcessor()\n",
        "X_train, X_test, y_train, y_test = processor.process(\n",
        "    '../data/WA_Fn-UseC_-Telco-Customer-Churn.csv',\n",
        "    target_column='Churn',\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "feature_engineer = FeatureEngineer()\n",
        "X_train_fe, X_test_fe, y_train_balanced, _ = feature_engineer.engineer_features(\n",
        "    X_train, X_test, y_train, apply_smote=True\n",
        ")\n",
        "\n",
        "# Load or train models\n",
        "trainer = ModelTrainer()\n",
        "try:\n",
        "    trainer.load_models('../models')\n",
        "    print(\"Models loaded from disk\")\n",
        "except:\n",
        "    trainer.train_models(X_train_fe, y_train_balanced)\n",
        "    print(\"Models trained\")\n",
        "\n",
        "# Evaluate models\n",
        "evaluator = ModelEvaluator()\n",
        "results_df = evaluator.evaluate_all_models(trainer.trained_models, X_test_fe, y_test)\n",
        "\n",
        "# Generate visualizations\n",
        "evaluator.plot_confusion_matrices(trainer.trained_models, X_test_fe, y_test)\n",
        "evaluator.plot_roc_curves(trainer.trained_models, X_test_fe, y_test)\n",
        "evaluator.plot_metrics_comparison(results_df)\n",
        "\n",
        "# SHAP analysis\n",
        "best_model_name = results_df.iloc[0]['Model']\n",
        "best_model = trainer.trained_models[best_model_name]\n",
        "feature_names = feature_engineer.feature_columns if hasattr(feature_engineer, 'feature_columns') else None\n",
        "evaluator.apply_shap_analysis(best_model, X_test_fe, best_model_name, feature_names=feature_names, max_samples=100)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
